diff --git a/dojo/settings/settings.dist.py b/dojo/settings/settings.dist.py
index 65754ac4c..e792f1788 100644
--- a/dojo/settings/settings.dist.py
+++ b/dojo/settings/settings.dist.py
@@ -657,6 +657,13 @@ def generate_url(scheme, double_slashes, user, password, host, port, path):
 # unique_id_from_tool or hash_code
 # Makes it possible to deduplicate on a technical id (same parser) and also on some functional fields (cross-parsers deduplication)
 DEDUPE_ALGO_UNIQUE_ID_FROM_TOOL_OR_HASH_CODE = 'unique_id_from_tool_or_hash_code'
+# makes it possible to deduplicate based on a custom configured set of attributes.
+DEDUPE_ALGO_ATTRIBUTE_CONFIG = 'configured_attributes'
+# Add the fields which should be considered when finding duplicate findings.
+# Add fields which are present in the DEDUPLICATION_ALLOWED_ATTRIBUTES list only.
+DEDUPLICATION_ATTRIBUTES = []
+# List of fields that are known to be usable in finding duplicates.
+DEDUPLICATION_ALLOWED_ATTRIBUTES = ['title', 'cwe', 'endpoints', 'offset', 'line', 'file_path', 'hash_code', 'sourcefile', 'param', 'url']
 
 # Choice of deduplication algorithm per parser
 # Key = the scan_type from factory.py (= the test_type)
diff --git a/dojo/utils.py b/dojo/utils.py
index 5bd785a12..641b04d57 100644
--- a/dojo/utils.py
+++ b/dojo/utils.py
@@ -107,6 +107,14 @@ def sync_dedupe(sender, *args, **kwargs):
                 deduplicate_hash_code(new_finding)
             elif(deduplicationAlgorithm == settings.DEDUPE_ALGO_UNIQUE_ID_FROM_TOOL_OR_HASH_CODE):
                 deduplicate_uid_or_hash_code(new_finding)
+            elif(deduplicationAlgorithm == settings.DEDUPE_ALGO_ATTRIBUTE_CONFIG) and hasattr(settings, 'DEDUPLICATION_ATTRIBUTES') and hasattr(settings, 'DEDUPLICATION_ALLOWED_ATTRIBUTES'):
+                configured_attributes = settings.DEDUPLICATION_ATTRIBUTES
+                if (all(elem in settings.DEDUPLICATION_ALLOWED_ATTRIBUTES for elem in configured_attributes)):
+                    deduplication_attr_config(new_finding, configured_attributes)
+                else:
+                    deduplicationLogger.debug("configuration error: some elements of DEDUPLICATION_ATTRIBUTES are not in the allowed list DEDUPLICATION_ALLOWED_ATTRIBUTES."
+                    "using legacy algorithm")
+                    deduplicate_legacy(new_finding)
             else:
                 deduplicate_legacy(new_finding)
         else:
@@ -285,6 +293,66 @@ def deduplicate_uid_or_hash_code(new_finding):
         break
 
 
+def deduplication_attr_config(new_finding, attributes):
+    if new_finding.test.engagement.deduplication_on_engagement:
+        existing_findings = Finding.objects.filter(
+            test__engagement=new_finding.test.engagement).exclude(
+            id=new_finding.id).exclude(duplicate=True)
+    else:
+        existing_findings = Finding.objects.filter(
+            test__engagement__product=new_finding.test.engagement.product).exclude(
+            id=new_finding.id).exclude(duplicate=True)
+
+    if 'offset' in attributes and 'line' in attributes:
+        attributes.remove('line')
+    for attr in attributes:
+        if attr == 'endpoints' or attr == 'offset':
+            continue
+        my_filter = {}
+        my_filter[attr] = getattr(new_finding, attr)
+        existing_findings = existing_findings.filter(**my_filter)
+
+    original_findings = []
+    findings_set_for_offset = []
+    for finding in existing_findings:
+        if is_deduplication_on_engagement_mismatch(new_finding, finding):
+            deduplicationLogger.debug(
+                'deduplication_on_engagement_mismatch, skipping dedupe.')
+            continue
+        if new_finding.dynamic_finding is True:
+            if 'endpoints' in attributes:
+                if finding.endpoints.count() != 0 and new_finding.endpoints.count() != 0:
+                    list1 = [e.host_with_port for e in new_finding.endpoints.all()]
+                    list2 = [e.host_with_port for e in finding.endpoints.all()]
+                    if all(x in list1 for x in list2):
+                        original_findings.append(finding)
+            else:
+                original_findings.append(finding)
+
+        elif new_finding.static_finding is True:
+            if 'offset' in attributes:
+                findings_set_for_offset.append(finding)
+                if finding.line == new_finding.line:
+                    original_findings.append(finding)
+            else:
+                original_findings.append(finding)
+
+    if not original_findings and new_finding.static_finding is True and 'offset' in attributes:
+        similar_findings_with_offset = list(filter(lambda i: abs(i.line - int(new_finding.line)) <= 100, findings_set_for_offset))
+        if similar_findings_with_offset:
+            for finding in similar_findings_with_offset:
+                finding.line_diff = abs(finding.line - int(new_finding.line))
+            original_findings = sorted(similar_findings_with_offset, key=lambda x: x.line_diff)
+
+    for find in original_findings:
+        try:
+            set_duplicate(new_finding, find)
+        except Exception as e:
+            deduplicationLogger.debug(str(e))
+            continue
+        break
+
+
 def set_duplicate(new_finding, existing_finding):
     if existing_finding.duplicate:
         raise Exception("Existing finding is a duplicate")
